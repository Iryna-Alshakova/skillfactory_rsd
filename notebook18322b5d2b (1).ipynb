{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install datasets\n#!pip install tokenizers==0.10.0rc1\n#!pip install wandb","metadata":{"id":"rJnHQn4htNIF","outputId":"a9944aca-2fa0-4a26-f0ce-1b32ccd9705a","execution":{"iopub.status.busy":"2021-10-05T16:17:10.215491Z","iopub.execute_input":"2021-10-05T16:17:10.216493Z","iopub.status.idle":"2021-10-05T16:17:10.235317Z","shell.execute_reply.started":"2021-10-05T16:17:10.216353Z","shell.execute_reply":"2021-10-05T16:17:10.234576Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nimport datasets\nimport tokenizers\nimport wandb\nfrom tqdm.auto import tqdm\n\nimport matplotlib.pyplot as plt","metadata":{"id":"IGJsIqgRtEn-","execution":{"iopub.status.busy":"2021-10-05T16:17:10.462291Z","iopub.execute_input":"2021-10-05T16:17:10.463250Z","iopub.status.idle":"2021-10-05T16:17:14.400064Z","shell.execute_reply.started":"2021-10-05T16:17:10.463188Z","shell.execute_reply":"2021-10-05T16:17:14.398976Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"text_dataset = datasets.load_dataset(\"imdb\")","metadata":{"id":"LUvFcan-tG5e","outputId":"3ab48820-0022-496a-ee5d-48f313eae75b","execution":{"iopub.status.busy":"2021-10-05T16:17:14.401924Z","iopub.execute_input":"2021-10-05T16:17:14.402704Z","iopub.status.idle":"2021-10-05T16:17:15.135389Z","shell.execute_reply.started":"2021-10-05T16:17:14.402662Z","shell.execute_reply":"2021-10-05T16:17:15.134615Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**IMDB** - это датасет по классификации эмоциональной окраски. Вам нужно предсказать положительный ли отзыв к фильму по его тексту. Это довольно простая задача и она хорошо решается даже линейными моделями. Для доступа к нему мы используем библиотеку `datasets` - она содержит в себе много интересных текстовых датасетов.\n\nТренировочная и тстовая части IMDB достаточно большие - каждая состоит из 25 тысяч примеров.","metadata":{"id":"1JrixvHI6teb"}},{"cell_type":"code","source":"text_dataset","metadata":{"id":"GL4eJ8hcwHA_","execution":{"iopub.status.busy":"2021-10-05T16:17:15.136604Z","iopub.execute_input":"2021-10-05T16:17:15.137430Z","iopub.status.idle":"2021-10-05T16:17:15.147077Z","shell.execute_reply.started":"2021-10-05T16:17:15.137383Z","shell.execute_reply":"2021-10-05T16:17:15.145995Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Как мы видим, классы сбалансированны, что позволяет использовать accuracy как простую и интерпретируемую метрику, хорошо показывающую качество модели.","metadata":{"id":"oUVSXa2r7oak"}},{"cell_type":"code","source":"train_labels = [e['label'] for e in text_dataset['train']]\nplt.hist(train_labels)\nplt.show()","metadata":{"id":"wbCkXd6_7VU_","execution":{"iopub.status.busy":"2021-10-05T16:17:15.150804Z","iopub.execute_input":"2021-10-05T16:17:15.151370Z","iopub.status.idle":"2021-10-05T16:17:17.067465Z","shell.execute_reply.started":"2021-10-05T16:17:15.151315Z","shell.execute_reply":"2021-10-05T16:17:17.066356Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"text_dataset['train']","metadata":{"id":"o04VWlLvwpDd","execution":{"iopub.status.busy":"2021-10-05T16:17:17.068783Z","iopub.execute_input":"2021-10-05T16:17:17.069029Z","iopub.status.idle":"2021-10-05T16:17:17.076772Z","shell.execute_reply.started":"2021-10-05T16:17:17.068999Z","shell.execute_reply":"2021-10-05T16:17:17.075754Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"text_dataset['train']['text'][0]","metadata":{"id":"SdqU7ZTiwrC9","execution":{"iopub.status.busy":"2021-10-05T16:17:17.079007Z","iopub.execute_input":"2021-10-05T16:17:17.079426Z","iopub.status.idle":"2021-10-05T16:17:17.175983Z","shell.execute_reply.started":"2021-10-05T16:17:17.079380Z","shell.execute_reply":"2021-10-05T16:17:17.174798Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Classification using a linear model\n\nЛинейная модель - очень сильный бейзлайн и в некоторых задачах классификации вам даже не надо идти дальше линейной модели - она уже достаточно хороша. А также её можно написать менее чем в 10 строчекю Поэтому всегда стоит начинать решение любой задачи с ленейного бейзлайна.\n\nДавайте вспомним библиотеку `sklearn` и напишем линейную модельку. Для векторизации текста мы будем использовать `TfidfVectorizer`, а в качестве модели `LogisticRegression`.\n","metadata":{"id":"bVemry0mziTa"}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"id":"fMU_c9pKxe0d","execution":{"iopub.status.busy":"2021-10-05T16:17:17.178141Z","iopub.execute_input":"2021-10-05T16:17:17.178470Z","iopub.status.idle":"2021-10-05T16:17:17.429250Z","shell.execute_reply.started":"2021-10-05T16:17:17.178433Z","shell.execute_reply":"2021-10-05T16:17:17.428482Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# TASK 1.1: create TfidfVectorizer object and fit it on out training set texts\n# Our implementation is 2 lines\n# YOUR CODE STARTS\nvectorizer = TfidfVectorizer()\nvectorizer.fit(text_dataset['train']['text'])\n\n# YOUR CODE ENDS","metadata":{"id":"i5qXrSEXztwW","execution":{"iopub.status.busy":"2021-10-05T16:17:17.430800Z","iopub.execute_input":"2021-10-05T16:17:17.431335Z","iopub.status.idle":"2021-10-05T16:17:24.820929Z","shell.execute_reply.started":"2021-10-05T16:17:17.431287Z","shell.execute_reply":"2021-10-05T16:17:24.819937Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# TASK 1.2:\n# 1. convert your texts to tf-idf vectors using .transform (training texts and test texts too)\n# 2. convert your labels into numpy arrays (both training and test labels)\n# Or implementatin is 4 lines\n\n# YOUR CODE STARTS\nX_train = vectorizer.transform(text_dataset['train']['text']).toarray()\ny_train = text_dataset['train']['label']\n\nX_test = vectorizer.transform(text_dataset['test']['text']).toarray()\ny_test = text_dataset['test']['label']\n# YOUR CODE ENDS","metadata":{"id":"RRkJP8yz2KPP","execution":{"iopub.status.busy":"2021-10-05T16:17:24.822665Z","iopub.execute_input":"2021-10-05T16:17:24.823181Z","iopub.status.idle":"2021-10-05T16:17:58.755945Z","shell.execute_reply.started":"2021-10-05T16:17:24.823134Z","shell.execute_reply":"2021-10-05T16:17:58.754813Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"len(X_train[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:17:58.759126Z","iopub.execute_input":"2021-10-05T16:17:58.759489Z","iopub.status.idle":"2021-10-05T16:17:58.765817Z","shell.execute_reply.started":"2021-10-05T16:17:58.759449Z","shell.execute_reply":"2021-10-05T16:17:58.765128Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# TASK 1.3: create LogisticRegression model object and fit the model\n# Our implementation is 2 lines\n# YOUR CODE STARTS\n\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train, y_train)\n\n\n# YOUR CODE ENDS","metadata":{"id":"gnWo2pLB1ho0","execution":{"iopub.status.busy":"2021-10-05T16:17:58.767172Z","iopub.execute_input":"2021-10-05T16:17:58.768065Z","iopub.status.idle":"2021-10-05T16:19:14.744269Z","shell.execute_reply.started":"2021-10-05T16:17:58.768013Z","shell.execute_reply":"2021-10-05T16:19:14.743263Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"А теперь мы используем нашу модель для того, чтобы предсказать классы на тестовом сете и считаем accuracy.","metadata":{"id":"RJRuIbVrqDpR"}},{"cell_type":"code","source":"predictions = model.predict(X_test)\n\nprint(type(predictions))\nprint(type(y_test))","metadata":{"id":"ZKCOA3SE3kSK","execution":{"iopub.status.busy":"2021-10-05T16:19:14.746063Z","iopub.execute_input":"2021-10-05T16:19:14.746688Z","iopub.status.idle":"2021-10-05T16:19:18.517383Z","shell.execute_reply.started":"2021-10-05T16:19:14.746637Z","shell.execute_reply":"2021-10-05T16:19:18.516374Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# note that we can use vector operations, because we deal with numpy tensors\naccuracy = (predictions == y_test).mean()\naccuracy","metadata":{"id":"pyO4CMzB3jds","execution":{"iopub.status.busy":"2021-10-05T16:19:18.519436Z","iopub.execute_input":"2021-10-05T16:19:18.520171Z","iopub.status.idle":"2021-10-05T16:19:18.538136Z","shell.execute_reply.started":"2021-10-05T16:19:18.520114Z","shell.execute_reply":"2021-10-05T16:19:18.537162Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**OMG so accurate, much machine learning**","metadata":{"id":"AENaKt7huHd1"}},{"cell_type":"markdown","source":"Давайте предскажем позитивны ли такие комментарии:","metadata":{"id":"5NpZecL4qqI5"}},{"cell_type":"code","source":"positive_comment = 'This movie is awesome!'\n\nvec = vectorizer.transform([positive_comment])\nmodel.predict(vec)","metadata":{"id":"XKlLVQUqsRlI","execution":{"iopub.status.busy":"2021-10-05T16:19:18.540205Z","iopub.execute_input":"2021-10-05T16:19:18.540883Z","iopub.status.idle":"2021-10-05T16:19:18.553874Z","shell.execute_reply.started":"2021-10-05T16:19:18.540828Z","shell.execute_reply":"2021-10-05T16:19:18.552895Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"negative_comment = 'This movie is awful!'\n\nvec = vectorizer.transform([negative_comment])\nmodel.predict(vec)","metadata":{"id":"CHtq-453t2Gb","execution":{"iopub.status.busy":"2021-10-05T16:19:18.556101Z","iopub.execute_input":"2021-10-05T16:19:18.556766Z","iopub.status.idle":"2021-10-05T16:19:18.568980Z","shell.execute_reply.started":"2021-10-05T16:19:18.556714Z","shell.execute_reply":"2021-10-05T16:19:18.568057Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Как мы увидели, даже такая простая модель может хорошо классифицировать текст (accuracy 0.9 - это довольно много, тк выборка сбалансированна).\n\nКстати, использование модели LinearSVM обычно работает даже лучше, чем логистическая регрессия. Рекомендуем попробовать и сравнить.\n\nЧто такое SVM: [тык](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)\n\nЕщё один простой метод улучшить линейную модель - использовать n-gram в вашем TF-IDF. Не забудьте указать параметр `max_features` (хорошое число 50 000), а то при большом количестве фичей модель может начать переобучаться.","metadata":{"id":"Xm2GycUTTjgW"}},{"cell_type":"markdown","source":"## LinearSVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\nsvm_model = LinearSVC(random_state=0).fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:19:18.571210Z","iopub.execute_input":"2021-10-05T16:19:18.571928Z","iopub.status.idle":"2021-10-05T16:19:29.404499Z","shell.execute_reply.started":"2021-10-05T16:19:18.571857Z","shell.execute_reply":"2021-10-05T16:19:29.403355Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"predictions = svm_model.predict(X_test)\naccuracy = (predictions == y_test).mean()\naccuracy","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:19:29.406598Z","iopub.execute_input":"2021-10-05T16:19:29.406973Z","iopub.status.idle":"2021-10-05T16:19:31.005094Z","shell.execute_reply.started":"2021-10-05T16:19:29.406923Z","shell.execute_reply":"2021-10-05T16:19:31.004134Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"positive_comment = 'This movie is awesome!'\n\nvec = vectorizer.transform([positive_comment])\nsvm_model.predict(vec)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:19:31.006995Z","iopub.execute_input":"2021-10-05T16:19:31.007549Z","iopub.status.idle":"2021-10-05T16:19:31.020972Z","shell.execute_reply.started":"2021-10-05T16:19:31.007516Z","shell.execute_reply":"2021-10-05T16:19:31.020088Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"negative_comment = 'This movie is awful!'\n\nvec = vectorizer.transform([negative_comment])\nsvm_model.predict(vec)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:19:31.026577Z","iopub.execute_input":"2021-10-05T16:19:31.028665Z","iopub.status.idle":"2021-10-05T16:19:31.037840Z","shell.execute_reply.started":"2021-10-05T16:19:31.028618Z","shell.execute_reply":"2021-10-05T16:19:31.036922Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## ngrams","metadata":{}},{"cell_type":"code","source":"vectorizer_n = TfidfVectorizer(ngram_range=(1,2), max_features=50000)\nvectorizer_n.fit(text_dataset['train']['text'])","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:20:37.841941Z","iopub.execute_input":"2021-10-05T16:20:37.843202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = vectorizer_n.transform(text_dataset['train']['text']).toarray()\ny_train = text_dataset['train']['label']\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = vectorizer_n.transform(text_dataset['test']['text']).toarray()\ny_test = text_dataset['test']['label']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\nsvm_model = LinearSVC(random_state=42).fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = svm_model.predict(X_test)\naccuracy = (predictions == y_test).mean()\naccuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_comment = 'This movie is awesome!'\n\nvec = vectorizer_n.transform([positive_comment])\nsvm_model.predict(vec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"negative_comment = 'This movie is awful!'\n\nvec = vectorizer_n.transform([negative_comment])\nsvm_model.predict(vec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}